{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Quick links\n"," ## If you want to skip to a particular result in this notebook, search for the following terms\n"," * \"What search engines were considered?\" / \"What devices were considered?\" / \"What query categories were considered?\"\n"," * \"What queries were made?\"\n"," * \"What Wikipedia links appeared in SERPs?\"\n"," * \"How many times does each Wikipedia link appear for each device and search engine?\"\n"," * \"How often did Wikipedia links appear in SERPs?\"\n"," * \"How often did Wikipedia links appear in certain locations of SERPs?\" (e.g. above-the-fold, in the right-hand column, etc.\n","\n"," For the code that calculate incidence rates, see analyze_links.py"]},{"cell_type":"markdown","metadata":{},"source":[" # Current data format\n"," Currently, the node.js scraping code (see collect.js)\n"," saves 3 result files per SERP scraped:\n"," * a .json file with\n"," device object used by puppeteer (\"device\"), date collection started (\"dateStr\"),\n"," date of collection (\"dataAtSave\"), user-specified query category (queryCat),\n"," file queries came from (\"queryFile\"), device name (\"deviceName\"),\n"," url accessed (\"link\"), the search engine or website (\"platform\"),\n"," the query made (\"target\"), and finally, a huge array of link elements (\"linkElements\")\n"," * a .png file that is a full screenshot of the SERP\n"," * a .mhtml snapshot of the website that can be opened in a web browser (this is experimental, apparently)\n","\n"," Files are named by datetime of script start to avoid accidental overwrite.\n","\n"," This script (analysis.py) includes code which stitches together a visual representation of\n"," links and their coordinates (obtained using getBoundingClientRect) alongside screenshots\n"," so search can perform visual validation -- compare the link representation (easy to do quant analyses)\n"," with the png representation and make sure they match up!"]},{"cell_type":"markdown","metadata":{},"source":[" # Looking at the data\n"," ## For a very quick glance, look at all the files in `quick_examples`\n"," ## Alternatively, can look through the entire `server_output` folder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# defaults\n","import json\n","import glob\n","from pprint import pprint\n","from collections import defaultdict\n","from urllib.parse import unquote\n","import os\n","\n","# scipy\n","import pandas as pd\n","import numpy as np\n","\n","# plotting / images\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import seaborn as sns\n","from PIL import Image\n","\n","# helpers for this project\n","from helpers import (\n","    infinite_defaultdict, recurse_print_infinitedict, extract,\n","    is_mobile,\n",")\n","from constants import CONSTANTS\n","\n","DO_COORDS = False\n","SAVE_PLOTS = False\n",""]},{"cell_type":"markdown","metadata":{},"source":[" The heavy lifting of analysis is in \"analyze_links.py\"\n"," `analyze_links_df` function takes a dataframe of links\n"," and calculates the width, height, and normalized coordinates of each link element.\n"," It extracts the domain from each link (using urlparse(link).netloc, see helpers.py).\n"," Finally, it calculates a variety of incidence rates: how often are various\n"," domains appearing in the full page, above-the-fold, in the right column, etc.\n"," Above-the-fold, right-hand, etc. are calculated using the constants defined above\n"," such as common viewport heights for desktop and mobile devices."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from analyze_links import analyze_links_df\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Which experiments should we load?\n","device_names = [\n","    'Chrome on Windows',\n","    'iPhone X',\n","]\n","\n","\n","search_engines = [\n","    'google',\n","    'bing',\n","    #'duckduckgo',\n","    # 'yahoo' not yet tested, but probably works decently well.\n","]\n","query_sets = [\n","    #'top',\n","    #'med',\n","    #'trend',\n","    #'covid19',\n","]\n","query_sets = 'all'\n","    \n",""]},{"cell_type":"markdown","metadata":{},"source":[" Below: load all the files from specified directory\n"," and put then load them into the \"full_df\" dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rows = []\n","outdir = 'covidout' # where are the files\n","for file in glob.glob(f'{outdir}/**/*.json', recursive=True):\n","    with open(file, 'r', encoding='utf8') as f:\n","        d = json.load(f)\n","    d['fileName'] = file\n","    rows.append(d)\n","full_df = pd.DataFrame(rows)\n","full_df.head(3)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ## \"What search engines were considered?\" / \"What devices were considered?\" / \"What query categories were considered?\"\n","print('Search engines and how many SERPs per search engine:')\n","print(full_df.platform.value_counts(), '\\n')\n","print('Device names and how many SERPs per device:')\n","print(full_df.deviceName.value_counts(), '\\n')\n","print('Query categories and how many SERPs per category:')\n","print(full_df.queryCat.value_counts(), '\\n')\n","if query_sets == 'all':\n","    query_sets = list(full_df.queryCat.unique())\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(query_sets)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Which SERPs are missing for each search engines?\n","pd.crosstab(full_df.platform, full_df.target)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["configs = []\n","for device_name in device_names:\n","    for search_engine in search_engines:\n","        for query_cat in query_sets:\n","            configs.append({\n","                'device_name': device_name,\n","                'search_engine': search_engine,\n","                'query_cat': query_cat,\n","            })\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ## \"What queries were made?\"\n","print('Queries made and how many SERPs per query:')\n","print(full_df.target.value_counts().sort_index())\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we will have one dataframe full of links for each combination of device_name / search_engine / query_cat\n","# in each df, each row corresponds to a single <a> link element\n","dfs = infinite_defaultdict()\n","# this three-key dict will be use the following sequence of keys: device_name, search_engine, query_cat\n","errs = []\n","for config in configs:\n","    device_name = config['device_name']\n","    search_engine = config['search_engine']\n","    query_cat = config['query_cat']\n","    sub = full_df[\n","        (full_df.deviceName == device_name) &\n","        (full_df.platform == search_engine) & \n","        (full_df.queryCat == query_cat) \n","    ]\n","    links_rows = []\n","    print(device_name, search_engine, query_cat)\n","    for i, row in sub.iterrows():\n","        linkElements = row.linkElements\n","        #print(row)\n","        try:\n","            for x in linkElements:\n","                x['target'] = row.target\n","                x['device_name'] = device_name\n","                x['search_engine'] = search_engine\n","                x['query_cat'] = query_cat\n","                x['file_name'] = row.fileName\n","                x['date_str'] = row.dateStr\n","            links_rows += linkElements\n","        except TypeError: # (linkElements is NaN, and therefore a float)\n","            print('error')\n","            errs.append(row)\n","        \n","    links_df = pd.DataFrame(links_rows)\n","\n","    dfs[device_name][search_engine][query_cat] = analyze_links_df(\n","        pd.DataFrame(links_df), is_mobile(device_name)\n","    )\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["errs\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for config in configs:\n","#     device_name = config['device_name']\n","#     search_engine = config['search_engine']\n","#     query_cat = config['query_cat']\n","#     tmp = dfs[device_name][search_engine][query_cat]\n","#     print(device_name, search_engine, query_cat)\n","#     print(tmp[tmp.error])\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Let's see which links are most common"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["concat_all_domains = []\n","for config in configs:\n","    device_name = config['device_name']\n","    search_engine = config['search_engine']\n","    query_cat = config['query_cat']\n","    concat_all_domains.append(\n","        dfs[device_name][search_engine][query_cat][['domain']]\n","    )\n","print('Top 20 domains in all SERPs collected:')\n","concatted_domains = pd.concat(concat_all_domains)['domain']\n","print(concatted_domains.value_counts()[:20])\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## What Wikipedia links appeared in SERPs?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('What are the Wikipedia links showing up on desktop?')\n","concat_wp_links = []\n","for config in configs:\n","    device_name = config['device_name']\n","    search_engine = config['search_engine']\n","    query_cat = config['query_cat']\n","    tmp = dfs[device_name][search_engine][query_cat]\n","    tmp = tmp[tmp.wikipedia_appears]\n","    tmp['norm_href'] = tmp.href.apply(\n","        lambda x: unquote(x.replace('http://', '').replace('https://', '').replace('.m.', '.'))\n","    )\n","    concat_wp_links.append(\n","        tmp\n","    )\n","concatted_wp_links = pd.concat(concat_wp_links)\n","concatted_wp_links['norm_href'].value_counts()\n","\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# How many times does each Wikipedia link appear for each device and search engine?\n","print('How many times does each Wikipedia link appear for each device and search engine?')\n","pd.crosstab(concatted_wp_links.norm_href, [concatted_wp_links.device_name, concatted_wp_links.search_engine])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('How many times does each Wikipedia link appear for each device and search engine?')\n","tmp_tab = pd.crosstab(concatted_wp_links.norm_href, [concatted_wp_links.search_engine, concatted_wp_links.target])\n","for i, row in tmp_tab.iterrows():\n","    print(row[row > 0])\n","    print()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# to stitch together image files:\n","# source: https://stackoverflow.com/questions/30227466/combine-several-images-horizontally-with-python\n",""]},{"cell_type":"markdown","metadata":{},"source":[" the below code creates visualization of our scraped links\n"," critically, this means we can compare our links (used for quant analysis)\n"," with actual screenshots of SERPs or actual SERPs.\n"," To facilitate even easier visual validation, the below code takes a sample of SERPS\n"," and stitches the coordinate visualization and SERP screenshot together.\n"," This is pretty slow, so there's a DO_COORDS flag to turn it off."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create the coordinate visualization\n","if DO_COORDS:\n","    for config in configs:\n","        device_name = config['device_name']\n","        search_engine = config['search_engine']\n","        query_cat = config['query_cat']\n","\n","        df = dfs[device_name][search_engine][query_cat]\n","        if type(df) == defaultdict:\n","            continue\n","        right_max = df['right'].max()\n","        bot_max = df['bottom'].max()\n","        ratio = bot_max / right_max\n","        k = f'{device_name}_{search_engine}_{query_cat}'\n","        print(k)\n","\n","        available_targets = list(full_df[\n","            (full_df.deviceName == device_name) & (full_df.platform == search_engine) & (full_df.queryCat == query_cat)\n","        ].target)\n","\n","        np.random.seed(0)\n","        chosen_ones = np.random.choice(available_targets, 5, replace=False)\n","        with open(f'reports/samples/{k}.txt', 'w', encoding='utf8') as f:\n","            f.write('\\n'.join(chosen_ones))\n","        for target in available_targets + [None]:\n","            if target:\n","                subdf = df[df['target'] == target]\n","            else:\n","                subdf = df\n","            file_name = subdf.file_name.iloc[0]\n","            if target:\n","                assert len(set(subdf.file_name)) == 1\n","            fig, ax = plt.subplots(1, 1, figsize=(CONSTANTS['figure_width'], CONSTANTS['figure_width'] * ratio))\n","            plt.gca().invert_yaxis()\n","            add_last = []\n","            for i_row, row in subdf.iterrows():\n","                if row.width == 0 or row.height == 0:\n","                    continue\n","                x = row['left']\n","                y = row['bottom']\n","                width = row['width']\n","                height = row['height']\n","                domain = row['domain']\n","\n","                if row['wikipedia_appears']:\n","                    # add it to the plot last so it is on top\n","                    add_last.append([domain, (x,y,), width, height])\n","                    \n","                else:\n","                    if row['platform_ugc']:\n","                        color = 'b'\n","                    # color internal search engine links as lightgray\n","                    if 'google' in domain or 'bing' in domain or 'duckduckgo' in domain:\n","                        color = 'lightgray'\n","                    else:\n","                        color = 'grey'\n","                    plt.annotate(domain, (x, y), color=color)\n","                    # Add the patch to the Axes\n","                    rect = matplotlib.patches.Rectangle((x,y),width,height,linewidth=1,edgecolor=color,facecolor='none')\n","                    ax.add_patch(rect)\n","            for domain, coords, width, height in add_last:\n","                plt.annotate(domain, coords, color='g')\n","                rect = matplotlib.patches.Rectangle(coords,width,height,linewidth=2,edgecolor=color,facecolor='none')\n","                ax.add_patch(rect)\n","\n","            # kp line = lefthand width border.\n","            kp_line = CONSTANTS['lefthand_width']\n","            if is_mobile(device_name):\n","                scroll_line = CONSTANTS['mobile_lines']['noscroll_mg']\n","            else:\n","                scroll_line = CONSTANTS['desktop_lines']['noscroll_mg']\n","            plt.axvline(kp_line, color='r', linestyle='-')\n","\n","            # show the right edge of the viewport\n","            plt.axvline(CONSTANTS['viewport_width'], color='k', linestyle='-')\n","            # show the page-fold\n","            plt.axhline(scroll_line, color='k', linestyle='-')\n","\n","            overlay_file_name = f'{file_name}_overlay.png'\n","            if SAVE_PLOTS:\n","                plt.savefig(overlay_file_name)\n","            #plt.savefig(f'reports/overlays/{k}_{target}_{file_name}.png')\n","\n","            plt.close()\n","            if target in chosen_ones:\n","                screenshot_path = file_name.replace('.json', '.png')\n","                # the overlay will be smaller\n","                #TODO\n","                try:\n","                    screenshot_img = Image.open(screenshot_path)\n","                    big_w, big_h = screenshot_img.size\n","                    overlay_img = Image.open(overlay_file_name)\n","                    small_w, small_h = overlay_img.size\n","                except FileNotFoundError: \n","                    continue\n","\n","                h_percent = (big_h/float(small_h))\n","                new_w = int((float(small_w) * float(h_percent)))\n","                resized_overlay = overlay_img.resize((new_w,big_h), Image.ANTIALIAS)\n","\n","                total_width = new_w + big_w\n","\n","                new_im = Image.new('RGB', (total_width, big_h))\n","\n","                x_offset = 0\n","                for im in (screenshot_img, resized_overlay):\n","                    new_im.paste(im, (x_offset,0))\n","                    x_offset += im.size[0]\n","                new_im.save(f'reports/samples/concat_{k}_{target}.png')\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# toss results in here for easy dataframe creation\n","row_dicts = [] # each row is one config: device_name / search_engine / query_cat (/geography?)\n","for config in configs:\n","    device_name = config['device_name']\n","    search_engine = config['search_engine']\n","    query_cat = config['query_cat']\n","\n","    print(device_name, search_engine, query_cat)\n","    df = dfs[device_name][search_engine][query_cat]\n","    if type(df) == defaultdict:\n","        continue\n","\n","    groupby = ['target', 'date_str']\n","    inc_rate = df.groupby(groupby).wikipedia_appears.agg(any).mean()\n","    inc = df.groupby(groupby).wikipedia_appears.agg(any).sum()\n","    total = df.groupby(groupby).wikipedia_appears.agg(any).count()\n","\n","    rh_inc_rate = df.groupby(groupby).wikipedia_appears_rh.agg(any).mean()\n","    lh_inc_rate = df.groupby(groupby).wikipedia_appears_lh.agg(any).mean()\n","\n","    if is_mobile(device_name):\n","        d = CONSTANTS['mobile_lines']\n","    else:\n","        d = CONSTANTS['desktop_lines']\n","    matches = set(df[df.wikipedia_appears == True]['target'])\n","\n","    row_dict = {\n","        'query_cat': query_cat,\n","        'search_engine': search_engine,\n","        'device_name': device_name,\n","        'inc_rate': inc_rate,\n","        'inc': inc,\n","        'total': total,\n","        'rh_inc_rate': rh_inc_rate,\n","        'lh_inc_rate': lh_inc_rate,\n","        'matches': matches\n","    }\n","    for name in d.keys():\n","        row_dict[f'{name}_inc_rate'] = df.groupby(groupby)[f'wikipedia_appears_{name}'].agg(any).mean()\n","        row_dict[f'lh_{name}_inc_rate'] = df.groupby(groupby)[f'wikipedia_appears_lh_{name}'].agg(any).mean()\n","    for domain in [\n","        'twitter', 'youtube',\n","        'facebook',\n","    ]:\n","        row_dict[f'{domain}_inc_rate'] = df.groupby(groupby)[f'{domain}_appears'].agg(any).mean() \n","\n","\n","    row_dicts.append(row_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df = pd.DataFrame(row_dicts)\n","results_df.head(3)\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## How often did Wikipedia links appear in SERPs? (tabular)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df[\n","    ['device_name', 'search_engine', 'query_cat', 'inc_rate', 'inc', 'total']\n","]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(results_df[results_df.search_engine == 'google'].matches.values)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FP = 'Full-page incidence'\n","RH = 'Right-hand incidence'\n","LH = 'Left-hand incidence'\n","AF_MG = 'Above-the-fold incidence'\n","AF_pretty = 'Above-the-fold incidence (lower bound - upper bound)'\n","\n","LH_AF_pretty = 'Left-hand above-the-fold incidence (lower bound - upper bound)'\n","LH_AF_LB = 'Left-hand above-the-fold incidence (lower bound)' \n","LH_AF_MG = 'Left-hand above-the-fold incidence'\n","LH_AF_UB = 'Left-hand above-the-fold incidence (upper bound)' \n","\n","AF_LB = 'Above-the-fold incidence (lower bound)'\n","AF_UB = 'Above-the-fold incidence (upper bound)'\n","\n","cols = [\n","    'device_name', 'search_engine', 'query_cat', 'inc_rate', 'rh_inc_rate',\n","    'lh_inc_rate',\n","]\n","for name in CONSTANTS['mobile_lines'].keys():\n","    cols += [f'{name}_inc_rate', f'lh_{name}_inc_rate']\n","print(cols)\n","\n","renamed = results_df[cols]\n","renamed.rename(columns={\n","    'device_name': 'Device', 'search_engine': 'Search Engine',\n","    'query_cat': 'Query Category', 'inc_rate': FP,\n","    'rh_inc_rate': RH,\n","    'lh_inc_rate': LH,\n","    'lh_noscroll_lb_inc_rate': LH_AF_LB,\n","    'lh_noscroll_mg_inc_rate': LH_AF_MG,\n","    'lh_noscroll_ub_inc_rate': LH_AF_UB,\n","    'noscroll_lb_inc_rate': AF_LB,\n","    'noscroll_mg_inc_rate': AF_MG,\n","    'noscroll_ub_inc_rate': AF_UB,\n","    'youtube_inc_rate': 'Youtube incidence rate',\n","    'twitter_inc_rate': 'Twitter incidence rate',\n","}, inplace=True)\n","\n","def pretty_bounds(row):\n","    mg = row[AF_MG]\n","    lb = row[AF_LB]\n","    ub = row[AF_UB]\n","    return f'{mg:.2f} ({lb:.2f} - {ub:.2f})'\n","\n","def pretty_bounds_lh(row):\n","    mg = row[LH_AF_MG]\n","    lb = row[LH_AF_LB]\n","    ub = row[LH_AF_UB]\n","    return f'{mg:.2f} ({lb:.2f} - {ub:.2f})'\n","\n","renamed[AF_pretty] = renamed.apply(pretty_bounds, axis=1)\n","renamed[LH_AF_pretty] = renamed.apply(pretty_bounds_lh, axis=1)\n","\n","renamed.replace(to_replace={\n","    'top': 'common',\n","    'med': 'medical',\n","    'trend': 'trending',\n","    'covid19': 'COVID-19'\n","}, inplace=True)\n","renamed\n","\n","renamed[[\n","    'Device', 'Search Engine', 'Query Category',\n","    FP, RH, LH, AF_pretty, LH_AF_pretty\n","]].to_csv('reports/main.csv', float_format=\"%.2f\", index=False)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["renamed[\n","    renamed.Device == 'desktop'\n","][[\n","    'Search Engine', 'Query Category',\n","    FP,  LH, RH, AF_pretty, LH_AF_pretty\n","]].to_csv('reports/desktop.csv', float_format=\"%.2f\", index=False)\n","renamed[\n","    renamed.Device == 'mobile'\n","][[\n","    'Search Engine', 'Query Category',\n","    FP, AF_pretty\n","]].to_csv('reports/mobile.csv', float_format=\"%.2f\", index=False)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["renamed.head(3)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["baseline_df = results_df[['device_name', 'search_engine', 'query_cat', 'twitter_inc_rate', 'youtube_inc_rate', 'facebook_inc_rate']]\n","baseline_df.rename(columns={\n","    'device_name': 'Device', 'search_engine': 'Search Engine',\n","    'query_cat': 'Query Category'\n","}, inplace=True)\n","baseline_df.to_csv('reports/other_domains.csv', float_format=\"%.2f\", index=False)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["melted = renamed.melt(id_vars=['Device', 'Search Engine', 'Query Category'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["melted.head(3)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## How often did Wikipedia links appear in SERPs? (visual)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["melted.rename(columns={\n","    'variable': 'y-axis',\n","    'value': 'Incidence rate',\n","}, inplace=True)\n","sns.set()\n","g = sns.catplot(\n","    x=\"Query Category\", y='Incidence rate',\n","    hue=\"Search Engine\", col=\"Device\", row='y-axis',\n","    palette=['g', 'b', 'y'],\n","    #order=['COVID-19'],\n","    #row_order=[FP, AF, RH],\n","    data=melted[melted['y-axis'] == FP], kind=\"bar\",\n","    height=3, aspect=1.5, ci=None,\n","    sharex=False,\n",")\n","if SAVE_PLOTS:\n","    plt.savefig('reports/FP_catplot.png', dpi=300)\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## How often did Wikipedia links appear in certain locations of SERPs?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# lh vs rh\n","g = sns.catplot(\n","    x=\"Query Category\", y='Incidence rate',\n","    hue=\"Search Engine\", col='y-axis', row='Device',\n","    col_order=[LH, RH],\n","    palette=['g', 'b', 'y'],\n","    #order=['common', 'trending', 'medical'],\n","    data=melted[\n","        ((melted['y-axis'] == LH) | (melted['y-axis'] == RH))\n","    ],\n","    kind=\"bar\",\n","    height=3, aspect=1.5, ci=None,\n","    sharex=False,\n",")\n","if SAVE_PLOTS:\n","    plt.savefig('reports/LHRH_catplot.png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# above-the fold middle ground\n","g = sns.catplot(\n","    x=\"Query Category\", y='Incidence rate',\n","    hue=\"Search Engine\", col=\"Device\", row='y-axis',\n","    palette=['g', 'b', 'y'],\n","    #order=['common', 'trending', 'medical'],\n","    #row_order=[FP, AF, RH],\n","    data=melted[melted['y-axis'] == AF_MG], kind=\"bar\",\n","    height=3, aspect=1.5, ci=None,\n","    sharex=False,\n",")\n","if SAVE_PLOTS:\n","    plt.savefig('reports/AF_catplot.png', dpi=300)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["g = sns.catplot(\n","    x=\"Query Category\", y='Incidence rate',\n","    hue=\"Search Engine\", col=\"Device\", row='y-axis',\n","    palette=['g', 'b', 'y'],\n","    #order=['common', 'trending', 'medical'],\n","    data=melted[melted['y-axis'] == LH_AF_MG], kind=\"bar\",\n","    height=3, aspect=1.5, ci=None,\n","    sharex=False,\n",")\n","if SAVE_PLOTS:\n","    plt.savefig('reports/LH_AF_catplot.png', dpi=300)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# max difference between search engines\n","results_df.groupby(['device_name', 'query_cat']).agg(lambda x: max(x) - min(x))['inc_rate']\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# max difference between devices\n","results_df.groupby(['search_engine', 'query_cat']).agg(lambda x: max(x) - min(x))['inc_rate']\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# diff between FP and AF\n","melted[\n","    (melted['y-axis'] == FP) | (melted['y-axis'] == AF_MG)\n","].groupby(['Device', 'Query Category', 'Search Engine']).agg(lambda x: max(x) - min(x))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# what's in the first but not in the second\n","\n","se_minus_se = {}\n","se_to_matches = {}\n","sub = results_df[(results_df.device_name == 'Chrome on Windows')]\n","for groupname, group in sub.groupby('query_cat'):\n","    print(groupname)\n","    for i, row in group.iterrows():\n","        se_to_matches[row.search_engine] = set(row.matches)\n","    se_to_matches\n","    for k1, v1 in se_to_matches.items():\n","        for k2, v2 in se_to_matches.items():\n","            if k1 == k2:\n","                continue\n","            se_minus_se[f'{k1}_{k2}'] = v1 - v2\n","    pprint(se_minus_se)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}